Пакет Python `apache-airflow` включает в себя несколько основных операторов, но у него нет компонентов для подключения к облаку. Это касается не только поставщиков облачных услуг, но и других внешних сервисов.

Например, чтобы установить операторы и соответствующие зависимости, необходимые для запуска `PostgresOperator`, установите пакет  [`apache-airflow-providers-postgres`](https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/index.html)
```bash
$ pip install apache-airflow-providers-postgres
```
Для Spark нужно установить пакет [`apache-airflow-providers-spark`](https://airflow.apache.org/docs/apache-airflow-providers-apache-spark/stable/index.html)
```bash
$ pip install apache-airflow-providers-apache-spark
```

Полный перечень пакетов можно найти на странице https://airflow.apache.org/docs/.

Пример `S3CopyObjectOperator`
```python
from airflow.providers.amazon.aws.operator.s3_copy_object import S3CopyObjectOperator

S3CopyObjectOperator(
	task_id="...",
	source_bucket_name="databucket",
	source_bucket_key="/data/{{ ds }}.json",
	dest_bucket_name="backupbucket",
	dest_bucket_key="/data/{{ ds }}-backup.json",
)
```

Сильной стороной Airflow является возможность запланировать ML-конвейер и при необходимости повторно запустить (частичные) конвейеры в случае появления новых данных или изменений в модели. Если необработанные данные постоянно обновляются, конвейер Airflow будет периодически повторно загружать необработанные данные и повторно развертывать модель, обученную на новых данных. Кроме того, специалист по обработке и анализу данных может настроить модель по своему вкусу, а конвейер Airflow может автоматически повторно развернуть модель без необходимости запускать что-либо вручную.
```python
import gzip
import io
import pickle

import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operator.s3_copy_object import    
    S3CopyObjectOperator
from airflow.providers.amazon.aws.operators.sagemaker_endpoint import 
    SageMakerEndpointOperator
from airflow.providers.amazon.aws.operators.sagemaker_training import 
    SageMakerTraing
from sagemaker.amazon.common import write_numpy_to_dense_tensor

dag = DAG(
	dag_id="chapter7_aws_handwritten_digits_classifier",
	schedule_interval=None,
	start_date=airflow.utils.dates.days_ago(3),
)

download_mnist_data = S3CopyObjectOperator(
	task_id="download_mnist_data",
	source_bucket_name="sagemaker-sample-data-eu-west-1",
	source_bucket_key="algorithms/kmeans/mnist/mnist.pkl.gz",
	dest_bucket_nname="[your-bucket]",
	dest_bucket_key="mnist.pkl.gz",
    dag=dag,
)

def _extract_mnist_data():
    s3hook = S3Hook()

    # Скачиваем набор данных S3 в память
    mnist_buffer = io.BytesIO()
    mnist_obj = s3hook.get_key(
        bucket_name="[your-bucket]",
        key="mnist.pkl.gz",
    )
    mnist_obj.download_fileobj(mnist_buffer)

    # Распаковываем gzip-файл, извлекаем набор данных, конвертируем и загружаем 
    # обратно в S3
    mnist_buffer.seek(0)
    with gzip.GzipFile(fileobj=mnist_buffer, mode="rb") as f:
        train_set, _, _ = pickle.loads(f.read(), encoding="latin1")
        output_buffer = io.BytesIO()
        write_numpy_to_dense_tensor(
            file=output_buffer,
            array=train_set[0],
            labels=train_set[1],
        )
        output_buffer.seek(0)
        s3hook.load_file_obj(
            output_buffer,
            key="mnist_data",
            bucket_name="[your-bucket]",
            replace=True,
        )

    extract_mnist_dat = PythonOperator(
        task_id="extract_mnist_data",
        python_callable=_extract_mnist_data,
        dag=dag,
    )

    sagemaker_train_model = SageMakerTrainingOperator(
        task_id="sagemaker_train_model",
        config={
            "TrainingnJobName": "mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}",
            "AlgorithmSpecification": ...
        },
        wait_for_completion=True,
        print_log=True,
        check_interval=10,
        dag=dag,
    )

    sagemaker_deploy_model = SageMakerEndpointOperator(
        task_id="sagemaker_deploy_model",
        wait_for_completion=True,
        config={
            "ModelName": "mnistclassifier-{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}",
            ...
        },
        dag=dag,
    )

download_mnist_data >> extract_mnist_data >> sagemaker_train_model >> sagemaker_deploy_model
```

С помощью команды интерфейса командной строки `airflow tasks test` можно запустить отдельную задачу для заданной даты выполнения
```bash
$ airflow tasks test \
    chapter7_aws_hanndwritten_digits_classifier \  # dag_id
	download_mnist_data \  # task_id
	2019-01-01
```

Команда `airflow tasks test` существует для запуска и проверки одной задачи и не записывает состояние в базу данных, однако для хранения журналов требуется база данных, поэтому мы должны инициализировать ее с помощью команды `airflow db init` [[Список литературы#^ae6dac]]<c. 176>.




