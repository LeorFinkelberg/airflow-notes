В Airflow ОАГ (ориентированный ациклический граф) определяется с помощью кода на языке Python в файлах, которые по сути являются сценариями Python, описывающими структуру соответствующего ОАГ. Таким образом, каждый файл ОАГ обычно описывает набор задач для данного графа и зависимости между задачами, которые затем анализируются Airflow для определения структуры графа. Помимо этого, эти файлы обычно содержат некоторые дополнительные метаданные о графе, сообщающие Airflow, как и когда он должен выполняться, и так далее.

Одни из преимуществ определения ОАГ Airflow в коде Python состоит в том, что этот программный подход обеспечивает большую гибкость при создании графа.

После того как вы определили структуру вашего конвейера (конвейеров) в виде ОАГ, Airflow позволяет вам определить параметр `schedule_interval` для каждого графа, который точно решает, когда ваш конвейер будет запущен Airflow. Таким образом, вы можете дать указание Airflow выполнять ваш граф каждый час, ежедневно, каждую неделю и т.д. Или даже использовать более сложные интервалы, основанные на выражениях, подобных Cron.

На высоком уровне Airflow состоит из трех основных компонентов:
- _планировщик Airflow_ -- анализирует ОАГ, проверяет параметр `schedule_interval` и (если все в порядке) начинает планировать задачи ОАГ для выполнения, передавая их воркерам Airlfow,
- _воркеры (workers) Airflow_ -- выбирают задачи, которые запланированы для выполнения, и выполняют их. Таким образом, они несут ответственность за фактическое "выполнение работы".
- _веб-сервер Airflow_ -- визуализирует ОАГ, анализируемые планировщиком, и предоставляет пользователям основной интерфейс для отслеживания выполнения графов и их результатов.

После того как задачи поставлены в очередь на выполнение, с ними уже работает _пул воркеров Airflow_, которые выполняют задачи _параллельно_ и отслеживают их результаты. Эти результаты передаются в _базу метаданных Airflow_, чтобы пользователи могли отслеживать ход выполнения задач и просматривать журналы с помощью веб-интерфейса Airflow (интерфейс, предоставляемый веб-сервером Airflow) [[Список литературы#^ae6dac]]<c. 39>.

Отладка сбоев задач довольно проста, поскольку представление в виде дерева позволяет увидеть, какие задачи не удалось выполнить, и изучить их журналы.

Причины не выбирать Airflow [[Список литературы#^ae6dac]]<c. 43>:
- Airflow в первую очередь предназначен для выполнения _повторяющихся_ или _задач по пакетной обработке данных_, а ==не потоковых рабочих нагрузок==,
- Airflow отдает предпочтение _конвейерам, структура_ которых _не меняется каждый раз при запуске_.

Преимущество Airflow состоит в том, что мы можем разделить большую работу, состояющую из одного или нескольких шагов, на отдельные "задачи", вместе образующие ОАГ. Несколько задач могут выполняться параллельно, и задачи могут использовать разные технологии. Например, сначала можно было бы запустить сценарий Bash, а затем сценарий на языке Python.

Пример ОАГ
```python
import json
import pathlib

import airflow
import requests
import requests.exceptions as requests_exceptions
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="download_rocket_launches",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval=None,
)

download_launches = BashOperator(
	task_id="download_launches",
	bash_command=(
	    "curl -o /tmp/launches.json -L"
	    "'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",
	),
	dag=dag,
)

def _get_pictures():
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")

get_pictures = PythonOperator(
	task_id="get_pictures",
	python_callable=_get_pictures,
	dag=dag,
)

notify = BashOperator(
	task_id="notify",
	bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
	dag=dag,
)

# задаем порядок выполнения задач: это и есть зависимости
download_launches >> get_pictures >> notify 
```

Класс `DAG` принимает два обязательных аргумента: `dag_id` (имя ОАГ) и `start_date` (дата и время, когда рабочий процесс должен быть запущен в первый раз).

Для `schedule_interval = None` означает, что _ОАГ не будет запускаться автоматически_ (его можно запустить вручную).

Каждый оператор выполняет одну единицу работы, а несколько операторов вместе образуют рабочий процесс или ОАГ в Airflow. В Airflow можно использовать побитовый сдвиг (`>>`) для определения зависимостей между задачами. Это гарантирует, что задача `get_pictures` запуститься только после успешного завершения `download_launches`, а задача `notify` запустится после успешного завершения `get_pictures`.

В Airflow операторы имеют единственную ответственность: они существуют для выполнения одной единицы работы. В документации Airflow термины _оператор_ и _задача_ используются как взаимозаменяемые. С точки зрения пользователя, они обозначают одно и то же и часто заменяют друг друга во время обсуждения. Операторы обеспечивают реализацию одной единицы работы.

_Задачи_ в Airflow можно рассматривать как _менеджер вокруг оператора_, который обеспечивает его правильное выполнение.

Использование `PythonOperator` всегда состоит из двух частей:
- мы определяем сам оператор (`get_pictures`),
- аргумент `python_callable` указывает на вызываемый объект, обычно функцию (`_get_pictures`).

При запуске оператор вызывается функция Python, которая будет выполнять функцию.
### Запуск ОАГ в Airflow

В минимальном варианте Airflow состоит из трех основных компонентов:
- планировщика,
- веб-сервера,
- базы данных.

Чтобы запустить Airflow, можно либо установить в окружении Python, либо запустить контейнер Docker.
#### Запуск Airflow в окружении Python

Чтобы установить и запустить Airflow как пакет Python из PyPI, следует
```bash
$ pip install apache-airflow
```

После установки Airflow запустите его, инициализировав базу метаданных (где хранится состояние Airflow), создав пользователя.
```bash
$ airflow db init
$ airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
$ cp download_rocket_launches.py ~/airflow/dags
$ airflow webserver
$ airflow scheduler
```

После инициализации базы данных с помощью `airflow db init` будет создан файл базы данных по пути `~/airflow/airflow.db`.

Планировщик и веб-сервер являются непрерывными процессами, которые держатт ваш терминал открытым, поэтому они запускаются в фоновом режиме с помощью `airflow webserver` и/или открывают второе окно терминала, чтобы запустить планировщик и веб-сервер по отдельности. После настройки можно перейти по адресу `http://localhost:8080` и выполнить вход с именем пользователя "admin" и паролем "admin".
#### Запуск Airflow в контейнерах Docker

Для запуска контейнеров Docker на вашем компьютере должен быть установлен Docker Engine.
```bash
$ docker run \
  -it \
  -p 8080:8080
  -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py
  --entrypoint=/bin/bash
  --name airflow
  apache/airflow:2.0.0-python3.8 \
  -c '( \
  airflow db init && \
    airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org )'
$ airflow webserver &
$ airflow scheduler
```

==В этом демонстрационном примере в одном контейнере запускается несколько процессов -- это плохо!== В настройках, предназначенных для промышленного окружения, _нужно запускать веб-сервер, планировщик и базу метаданных в отдельных контейнерах_ [[Список литературы#^ae6dac]]<c. 57>.

В Airflow можно запланировать запуск ОАГ через определенные промежутки времени, например, один раз в час, день или месяц. Это можно контролировать, если задать аргумент `schedule_interval`
```python
dag = DAG(
	dag_id="download_rocket_launches",
	start_date=airflow.utils.dates.days_ago(14),
	schedule_interval="@daily",  # псевдоним 0 0 * * *
)
```

Airflow будет запускать этот рабочий процесс один раз в день. Когда мы задали `schedule_interval` значение `@daily`, Airflow знал, что должен запускать этот ОАГ один раз в день. Учитывая `start_date`, предоставленный ОАГ 14 дней назад, это означает, что время с момента 14 дней назад до настоящего момента можно разделить на 14 равных интервалов одного дня. Поскольку и дата начала, и дата окончания этих 14 итервалов находится в прошлом, они начнут выполняться, как только мы предоставим программе Airflow аргумент `schedule_interval`.

Если задача упала, то после исправления проблемы, ее можно перезапустить. Приятная особенность Airflow состоит в том, что вы можете выполнить перезапуск с момента сбоя и далее, без необходимости перезапуска каких бы то ни было ранее выполненных задач.
