В Airflow ОАГ (ориентированный ациклический граф) определяется с помощью кода на языке Python в файлах, которые по сути являются сценариями Python, описывающими структуру соответствующего ОАГ. Таким образом, каждый файл ОАГ обычно описывает набор задач для данного графа и зависимости между задачами, которые затем анализируются Airflow для определения структуры графа. Помимо этого, эти файлы обычно содержат некоторые дополнительные метаданные о графе, сообщающие Airflow, как и когда он должен выполняться, и так далее.

Когда мы запускаем ежедневный пайплайн, то с большой вероятностью захотим обрабатывать данные за вчера. Именно поэтому execution_date будет равен левой границе интервала, за которой мы обрабатываем данные. Например, сегодняшний запуск, который стартовал в час ночи по UTC, получит в качестве execution_date вчерашнюю дату. В случае ежечасного пайплайна ситуация такая же: для запуска пайплайна в 6 утра время в execution_date будет равно 5 часам утра. Это мысль поначалу не очень очевидна, но тем не менее, она очень осмысленная и важная.

Одни из преимуществ определения ОАГ Airflow в коде Python состоит в том, что этот программный подход обеспечивает большую гибкость при создании графа.

Каждая строчка страшной таблицы на Tree View – это одна задача, каждый столбец – один запуск пайплайна. На их пересечении – квадратик с запуском определенной задачи за определенную дату. Если на него нажать, появляется меню, где можно посмотреть детальную информацию и логи этой задачи, запустить или перезапустить ее, а также пометить её как успешную или неудачную.

Вообще, Scheduler – это самое интересное и одновременно самое узкое место в архитектуре Airflow:
- Первый нюанс заключается в том, что ==в один момент времени может работать только один инстанс Scheduler’a==. Это значит, что на текущий момент невозможен режим работы в High Availability (разработчики планируют добавить Scheduler HA в Airflow версии 2.0).
- Второй нюанс: ==в определённый момент на запуск могут отправиться несколько пайплайнов, из-за чего фактический старт задач может откладываться на несколько минут==. Бывалые дяденьки рассказывают, что запаздывания могут затягиваться на полчаса-час или больше, но лично я с таким не сталкивался.

До некоторых пор запаздывание тюнится параметрами конфиг-файла Airflow, но лаг на запуск все равно остается. Из этого следует, что ==Airflow – это не про real-time обработку данных==. Если поступить неосторожно и указать слишком частый интервал запуска (раз в пару минут), можно добиться запаздывания вашего пайплайна. Опыт показывает, что ==раз в 5 минут – уже достаточно часто, а некоторые не советуют запускать пайплайн раз в 10 минут==. У нас есть пара пайплайнов, стартующих раз в 10 минут, они довольно простые и до сих пор проблем с ними не было.

После того как вы определили структуру вашего конвейера (конвейеров) в виде ОАГ, Airflow позволяет вам определить параметр `schedule_interval` для каждого графа, который точно решает, когда ваш конвейер будет запущен Airflow. Таким образом, вы можете дать указание Airflow выполнять ваш граф каждый час, ежедневно, каждую неделю и т.д. Или даже использовать более сложные интервалы, основанные на выражениях, подобных Cron.

На высоком уровне Airflow состоит из трех основных компонентов:
- _планировщик Airflow_ -- анализирует ОАГ, проверяет параметр `schedule_interval` и (если все в порядке) начинает планировать задачи ОАГ для выполнения, передавая их воркерам Airlfow,
- _воркеры (workers) Airflow_ -- выбирают задачи, которые запланированы для выполнения, и выполняют их. Таким образом, они несут ответственность за фактическое "выполнение работы".
- _веб-сервер Airflow_ -- визуализирует ОАГ, анализируемые планировщиком, и предоставляет пользователям основной интерфейс для отслеживания выполнения графов и их результатов.

После того как задачи поставлены в очередь на выполнение, с ними уже работает _пул воркеров Airflow_, которые выполняют задачи _параллельно_ и отслеживают их результаты. Эти результаты передаются в _базу метаданных Airflow_, чтобы пользователи могли отслеживать ход выполнения задач и просматривать журналы с помощью веб-интерфейса Airflow (интерфейс, предоставляемый веб-сервером Airflow) [[Список литературы#^ae6dac]]<c. 39>.

Отладка сбоев задач довольно проста, поскольку представление в виде дерева позволяет увидеть, какие задачи не удалось выполнить, и изучить их журналы.

Причины не выбирать Airflow [[Список литературы#^ae6dac]]<c. 43>:
- Airflow в первую очередь предназначен для выполнения _повторяющихся_ или _задач по пакетной обработке данных_, а ==не потоковых рабочих нагрузок==,
- Airflow отдает предпочтение _конвейерам, структура_ которых _не меняется каждый раз при запуске_.

Преимущество Airflow состоит в том, что мы можем разделить большую работу, состояющую из одного или нескольких шагов, на отдельные "задачи", вместе образующие ОАГ. Несколько задач могут выполняться параллельно, и задачи могут использовать разные технологии. Например, сначала можно было бы запустить сценарий Bash, а затем сценарий на языке Python.

Пример ОАГ
```python
import json
import pathlib

import airflow
import requests
import requests.exceptions as requests_exceptions
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
    dag_id="download_rocket_launches",
    start_date=airflow.utils.dates.days_ago(14),
    schedule_interval=None,
)

download_launches = BashOperator(
	task_id="download_launches",
	bash_command=(
	    "curl -o /tmp/launches.json -L"
	    "'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",
	),
	dag=dag,
)

def _get_pictures():
    pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

    with open("/tmp/launches.json") as f:
        launches = json.load(f)
        image_urls = [launch["image"] for launch in launches["results"]]
        for image_url in image_urls:
            try:
                response = requests.get(image_url)
                image_filename = image_url.split("/")[-1]
                target_file = f"/tmp/images/{image_filename}"
                with open(target_file, "wb") as f:
                    f.write(response.content)
                print(f"Downloaded {image_url} to {target_file}")
            except requests_exceptions.MissingSchema:
                print(f"{image_url} appears to be an invalid URL.")
            except requests_exceptions.ConnectionError:
                print(f"Could not connect to {image_url}.")

get_pictures = PythonOperator(
	task_id="get_pictures",
	python_callable=_get_pictures,
	dag=dag,
)

notify = BashOperator(
	task_id="notify",
	bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
	dag=dag,
)

# задаем порядок выполнения задач: это и есть зависимости
download_launches >> get_pictures >> notify 
```

Класс `DAG` принимает два обязательных аргумента: `dag_id` (имя ОАГ) и `start_date` (дата и время, когда рабочий процесс должен быть запущен в первый раз).

Для `schedule_interval = None` означает, что _ОАГ не будет запускаться автоматически_ (его можно запустить вручную).

Каждый оператор выполняет одну единицу работы, а несколько операторов вместе образуют рабочий процесс или ОАГ в Airflow. В Airflow можно использовать побитовый сдвиг (`>>`) для определения зависимостей между задачами. Это гарантирует, что задача `get_pictures` запуститься только после успешного завершения `download_launches`, а задача `notify` запустится после успешного завершения `get_pictures`.

В Airflow операторы имеют единственную ответственность: они существуют для выполнения одной единицы работы. В документации Airflow термины _оператор_ и _задача_ используются как взаимозаменяемые. С точки зрения пользователя, они обозначают одно и то же и часто заменяют друг друга во время обсуждения. Операторы обеспечивают реализацию одной единицы работы.

_Задачи_ в Airflow можно рассматривать как _менеджер вокруг оператора_, который обеспечивает его правильное выполнение.

Использование `PythonOperator` всегда состоит из двух частей:
- мы определяем сам оператор (`get_pictures`),
- аргумент `python_callable` указывает на вызываемый объект, обычно функцию (`_get_pictures`).

При запуске оператор вызывается функция Python, которая будет выполнять функцию.
### Запуск ОАГ в Airflow

В минимальном варианте Airflow состоит из трех основных компонентов:
- планировщика,
- веб-сервера,
- базы данных.

Чтобы запустить Airflow, можно либо установить в окружении Python, либо запустить контейнер Docker.
#### Запуск Airflow в окружении Python

Чтобы установить и запустить Airflow как пакет Python из PyPI, следует
```bash
$ pip install apache-airflow
```

После установки Airflow запустите его, инициализировав базу метаданных (где хранится состояние Airflow), создав пользователя.
```bash
$ airflow db init
$ airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org
$ cp download_rocket_launches.py ~/airflow/dags
$ airflow webserver
$ airflow scheduler
```

После инициализации базы данных с помощью `airflow db init` будет создан файл базы данных по пути `~/airflow/airflow.db`.

Планировщик и веб-сервер являются непрерывными процессами, которые держатт ваш терминал открытым, поэтому они запускаются в фоновом режиме с помощью `airflow webserver` и/или открывают второе окно терминала, чтобы запустить планировщик и веб-сервер по отдельности. После настройки можно перейти по адресу `http://localhost:8080` и выполнить вход с именем пользователя "admin" и паролем "admin".
#### Запуск Airflow в контейнерах Docker

Для запуска контейнеров Docker на вашем компьютере должен быть установлен Docker Engine.
```bash
$ docker run \
  -it \
  -p 8080:8080
  -v /path/to/dag/download_rocket_launches.py:/opt/airflow/dags/download_rocket_launches.py
  --entrypoint=/bin/bash
  --name airflow
  apache/airflow:2.0.0-python3.8 \
  -c '( \
  airflow db init && \
    airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org )'
$ airflow webserver &
$ airflow scheduler
```

==В этом демонстрационном примере в одном контейнере запускается несколько процессов -- это плохо!== В настройках, предназначенных для промышленного окружения, _нужно запускать веб-сервер, планировщик и базу метаданных в отдельных контейнерах_ [[Список литературы#^ae6dac]]<c. 57>.

В Airflow можно запланировать запуск ОАГ через определенные промежутки времени, например, один раз в час, день или месяц. Это можно контролировать, если задать аргумент `schedule_interval`
```python
dag = DAG(
	dag_id="download_rocket_launches",
	start_date=airflow.utils.dates.days_ago(14),
	schedule_interval="@daily",  # псевдоним 0 0 * * *
)
```

Airflow будет запускать этот рабочий процесс один раз в день. Когда мы задали `schedule_interval` значение `@daily`, Airflow знал, что должен запускать этот ОАГ один раз в день. Учитывая `start_date`, предоставленный ОАГ 14 дней назад, это означает, что время с момента 14 дней назад до настоящего момента можно разделить на 14 равных интервалов одного дня. Поскольку и дата начала, и дата окончания этих 14 итервалов находится в прошлом, они начнут выполняться, как только мы предоставим программе Airflow аргумент `schedule_interval`.

Если задача упала, то после исправления проблемы, ее можно перезапустить. Приятная особенность Airflow состоит в том, что вы можете выполнить перезапуск с момента сбоя и далее, без необходимости перезапуска каких бы то ни было ранее выполненных задач.
#### Рекомендации 

Ниже я привел несколько советов, которые помогут не выстрелить себе в ногу при использовании Airflow:  
- Полезно держать каждый пайплайн (или генератор пайплайнов, об этом ниже) в отдельном файле. Я сразу знаю, в какой файл нужно залезть, чтобы посмотреть на нужный пайплайн или генератор.
- Хорошей практикой является подход, когда пайплайн состоит из задач, которые максимально атомарны и идемпотентны. Конечно, можно создать одну задачу со здоровенным баш-скриптом, делающим все на свете. Но при разделенных задачах мы быстрее поймем, когда что-то пошло не так, и сможем быстро локализовать проблему. Про идемпотентность я уже говорил: в случае перезапуска можно быть уверенным, что мы обрабатываем ровно ту порцию данных, которую хотим обработать.
- Следующий неочевидный совет – при изменении `schedule_interval` или `start_date` нужно менять `dag_id`. Это связано с тем, что в базе метаданных Airflow уже есть запись о том, что такой-то пайплайн запускается тогда-то. При изменении расписания в таблицу DAGS добавляется еще одна строчка, что сводит с ума Scheduler, потому что он видит два пайплайна с разным расписанием. Для решения этой проблемы мы раньше указывали версию в названии пайплайнов, но сейчас мы перешли на подход, при котором вшиваем расписание прямо в dag_id. Таким образом пайплайн автоматически получает новое имя, и этим не нужно заниматься вручную.
- При создании пайплайна можно передать ему параметр `catchup`. Если его значение `True`, то Airflow начнет создавать запуски пайплайна для каждого интервала, начиная от `start_date` до текущей даты. Иногда это совсем не то, что вам нужно. При значении `False` Airflow создаст только один запуск за последний доступный интервал. Самое интересное, что по умолчанию в Airflow этот параметр равен True (значение по умолчанию задается в конфиг-файле).
- И последнее – стоит делать инициализацию пайплайнов максимально легкой. Шедулер раз в определенный промежуток времени приходит в директорию с пайплайнами, ищет все python файлы, в которых присутствуют слова airflow и DAG, и запускает код в этих файлах, после чего ищет все созданные объекты класса DAG. Тяжелая логика в файлах с пайплайнами может сильно повлиять на производительность. Например, если для инициализации пайплайна мы сначала ходим в базу данных, а она вдруг начинает висеть или таймаутить. Или сначала ходим в REST API, но вызов requests.get() без указания таймаута вдруг начинает бесконечно висеть.

В Spark есть несколько способов для запуска задания:
- использовать `SparkSubmitOperator` -- для этого требуется утилита spark-submit и режим yarn-client на компьютере, где работает Airflow, чтобы найти экземпляр Spark,
- применить `SSHOperator`,
- использовать `SimpleHTTPOperator`.

Ключ к работе с любым оператором в Airflow -- это чтение документации, чтобы выяснить, какие аргументы предоставить. 

Простой пример выполнения HTTP-запроса
```python
import requests

session = requests.Session()

# метод get
reponse = session.get("http://localhost:5000/ratings")
```

Метод `get` также позволяет передавать дополнительные аргументы, такие как параметры (например, даты начала/окончания), чтобы включить их в запрос
```python
response = session.get(
	"http://localhost:5000/ratings",
	params = {
        "start_date": "2019-01-01",
        "end_date": "2019-01-02",
	}
)
```
Вызов `get` вернет объект ответа, представляющий собой результат запроса. Этот объект можно использовать для проверки успешности запроса с помощью метода `raise_for_status` , который возбуждает исключение, если запрос вернул неожиданный код состояния. Мы можем получить доступ к результату запроса с помощью атрибута `content` или с помощью метода `json`
```python
response.raise_for_status()
response.json()
```
Нужно еще включить аутентификацию
```python
session.auth = (movielens_user, movielens_password)
```
Это гарантирует, что сеанс запросов включает сюда аутентификацию по имени пользователя и паролю с запросами.

Чтобы это выглядело более приемлемо с точки зрения конфигурации, мы также можем указать имя пользователя и пароль и различные части нашего URL-адреса с использованием переменных окружения
```python
MOVIELENS_HOST = os.environ.get("MOVIELENS_HOST", "movielens")
MOVIELENS_SCHEMA = os.environ.get("MOVIELENS_SCHEMA", "http")
MOVIELENS_PORT = os.environ.get("MOVIESLENS_PORT", "5000")
MOVIELENS_USER = os.environ["MOVIELENS_USER"]
MOVIELENS_PASSWORD = os.environ["MOVIELENS_PASSWORD"]

def _get_session():
    session = requests.Session()
    session.auth = (MOVIELENS_USER, MOVIELENS_PASSWORD)

    base_url = f"{MOVIELENS_SCHEMA}://{MOVIELENS_HOST}:{MOVIELENS_PORT}"

    return session, base_url

session, base_url = _get_session()
```

Один из способов спрятать сложную логику взаимодействия с API -- инкапсулировать ее в _хук_, который можно будет использовать повторно [[Список литературы#^ae6dac]]<c. 200>.

По соглашению в корне проекта создается папка `tests/`, которая _содержит все тесты и зеркально отображает ту же структуру каталогов, что и в остальной части проекта_ [[Список литературы#^ae6dac]]<c. 225>.

Все файлы зеркально отражают имена файлов, которые тестируются, с префиксом `test_` или постфиксом `_test`. `pytest` просматривает заданные каталоги и ищет файлы с `test_*` или `*_test`. 

В Pytest имеется набор вспомогательных плагинов, которые упрощают использование таких понятий, как имитация. Для этого можно установить пакет `pytest-mock`
```bash
$ pip install pytest-mock
```

`pytest-mock` -- это пакет Python, предоставляющий небольшую удобную обертку для встроенного пакета имитаций. Чтобы использовать его, передайте вашей функции аргумент "mocker" (тип `pytest_mock.MockFixture`), который является точкой входа для использования чего-либо в пакете `pytest-mock`
```python
def test_movielenspopularityoperator(mocker):
    mocker.patch.object(
        MovielensHook,
        "get_connection",
        return_value=Connection(
            conn_id="test",
            login="airflow",
            password="airflow",
        ),
    )
    task = MovielensPopularityOperator(
        task_id="test_id",
        conn_id="test",
        start_date="2015-01-01",
        end_date="2015-01-03",
    )
    result = task.execution(context=None)
    assert len(result) == 5
```

С помощью этого кода к вызову метода `get_connection()` в `MovielensHook` применяется обезьяний патч" (заменяя его функциональность во время выполнения, чтобы вернуть данные объект вместо запроса в базу метаданных Airflow). Вместо этого возвращается предопределенный ожидаемый объект `Connection`.
```python
def test_movielenspopularityoperator(mocker):
    mock_get = mocker.patch.object(
        # объект, к которому нужно применить патч
        MovielensHook,
        # функция, к которой нужно применить патч
        "get_connection",
        # возвращаемое значение
        return_value=Connection(
            conn_id="test",
            login="airflow",
            password="airflow",
        )
    )
    task = MovielensPopularityOperator(...)
```

NB! Имитировать нужно место вызова, а не место определения [[Список литературы#^ae6dac]]<c. 238>.

В Python есть модуль `tempfile` для задач, связанных с временным хранилищем. Он не оставляет остатков в файловой системе, поскольку каталог и его содержимое стираются после использования. `pytest` предоставляет удобную точку доступа к этому модулю, `tmp_dir` (дает объект `os.path`) и `tmp_path` (дает объект `pathlib`)
```python
import csv
import json
from pathlib import Path

from airflowbook.operators.json_to_csv_operator import JsonToCsvOperator

def test_json_to_csv_operator(tmp_path: Path):
    input_path = tmp_path / "input.json"
    output_path = tmp_path / "output.csv"
    input_data = [
        {"name": "bob", "age": "41", "sex": "M"},
        {"name": "alice", "age": "24", "sex": "F"},
        {"name": "carol", "age": "41", "sex": "F"},
    ]

    with open(input_path, "w") as f:
        f.write(json.dumps(input_data))

    operator = JsonToCsvOperator(
        task_id="test",
        input_path=input_path,
        output_path=output_path,
    )
    operator.execute(context={})

    with open(output_path) as f:
        reader = csv.DictReader(f)
        result = [dict(row) for row in reader]

    assert result == input_data
```

После теста `tmp_path` и его содержимое удаляются. По умолчанию область действия фикстур -- это каждая тестовая функция.

В общем случае нельзя просто запустить `operator.execute(context={})`, потому что оператору все-таки нужен контекст для выполнения кода.

По умолчанию `AIRFLOW_HOME=~/airflow`. При тестировании понадобиться база метаданных. Существует несколько подходов к работе с базой метаданных во время тестирования:
- Можно было бы имитировать каждый вызов базы данных, при запросе учетных  для подключения. Однако, выглядеть это будет очень громоздко. 
- Более практичный подход -- запустить реальную базу метаданных, к которой Airflow может отправлять запросы при запуске тестов.

Во втором случае нужно выполнить команду `airflow db init`, которая инициализирует базу данных. Это будет база даннах SQLite, которая хранится в `~/airflow/airflow.db`. Если задать значение для переменной `AIRFLOW_HOME`, Airflow сохранит базу данных в этом каталоге.