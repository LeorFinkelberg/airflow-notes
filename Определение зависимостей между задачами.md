### Ветвление

#### Ветвление внутри ОАГ

Такое поведение, определяющее, когда выполняются задачи, котролируется так называемыми правилами триггеров в Airflow. Правила триггеров можно определить для отдельных задач с помощью аргумента `trigger_rule`, который можно передать любому оператору. По умолчанию для правил триггера установлено значение `all_success`. Это означает, что все родители соответствующей задачи должны завершиться успешно до того, как задачу можно будет запустить. Такого никогда не происходит при использовании `BranchPythonOperator`, поскольку он пропускает все задачи, не выбранные веткой. 

Чтобы исправить ситуацию, можно изменить правило триггеров `join_datasets`, чтобы оно сработало, если одна из вышестоящих задач пропущена. Один из способов это сделать -- изменить правило на `none_field`. Это указывает на то, что задача должна выполняться, как только все ее родительские задачи будут выполнены и ни одна из них не завершилась сбоем.

Один из недостатков такого подхода состоит в том, что теперь у нас есть 3 ребра, входящих в задачу `join_datasets`. На самом деле это не отражает характер нашего потока, в котором мы, по сути, хотим получить данные о продажах и погоде, а затем загрузить эти два источника данных в `join_datasets` . По этой причине многие предпочитают сделать условие ветвления более явным, добавляя фиктивную задачу, которая объединяет разные ветви, прежде чем продолжить работу с ОАГ.

Чтобы сделать структуру ветвления более понятной, можно добавить дополнительную задачу соединения после ветви, которая связывает разные ветви перед продолжением работы с остальной частью ОАГ. У этой задачи имеется дополнительное преимущество, заключающееся в том, что вам не нужно изменять какие-либо правила триггеров для других задач в ОАГ, поскольку вы можете задать необходимое правило для задачи соединения. Обратите внимание: это означает, что вам больше не нужно задавать правило триггеров для задачи `join_datasets`.

Чтобы добавить такую фиктивную задачу в ОАГ, можно использовать встроенный оператор `DummyOperator`
```python
# Добавление фиктивной задачи соединения для ясности
from airflow.operators.dummy import DummyOperator

join_branch = DummyOperator(
	task_id="join_erp_branch",
	trigger_rule="none_failed",
)

[clean_sales_old, clean_sales_new] >> join_branch
join_branch >> join_datasets
```
Это изменение также означает, что нам больше не нужно изменять правило триггеров для задачи `join_datasets`, что делает нашу ветку более автономной по сравнению с исходной.
#### Условные задачи

Airflow также предоставляет другие механизмы для пропуска определенных задач в ОАГ в зависимости от определенных условий. Это позволяет запускать определенные задачи только при наличии определенных наборов данных или лишь в том случае, если ваш ОАГ выполняется для самой последней даты выполнения.

Чтобы реализовать условное развертывание, нужно сделать саму задачу развертывания условной. Это означает, что она выполняется только на основе заранее определенного условия (в этом случае является ли запуск ОАГ самым последним). В Airflow можно сделать задачи условными, добавив задачу в ОАГ, который проверяет указанное условие и обеспечивает пропуск всех нижестоящих задач в случае невыполнения условия. 

Кроме того, можно сделать развертывание условным, добавив задачу, которая проверяет, является ли текущее выполнение самым последним выполнением ОАГ, и добавив задачу развертывания после этой задачи
```python
def _latest_only(**context):
    ...

latest_only = PythonOperator(
	task_id="latest_only",
	python_callable=_latest_only,
	dag=dag,
)

latest_only >> deploy_model
```

Затем нам нужно заполнить функцию `_latest_only`, чтобы убедиться, что нижестоящие задачи пропускаются, если `execution_date` не принадлежит самому последнему запуску. Для этого нужно проверить дату выполнения и, при необходимости, вызвать исключение `AirflowSkipException` из нашей функции. Это способ Airflow, позволяющий указать, что условие и все его нижестоящие задачи должны быть пропущены, тем самым пропуская развертывание.
```python
from airflow.exceptions import AirflowSkipException

def _latest_only(**context):
    left_window = context["dag"].following_schedule(context["execution_date"])
    right_window = context["dag"].following_schedule(left_window)

    now = pendulum.now()
    if not left_window < now <= right_window:
        raise AirflowSkipException("Not the most recent run!")
```
Как это работает? По сути, происходит следующее: когда наша задача условия (`latest_only`) возбуждает исключение `AirflowSkipException`, задача завершается, и ей назначается состояние пропуска. Затем Airflow проверяет правила триггеров всех нижестоящих задач, чтобы определить, должны ли они запускаться. В этом случае у нас есть только одна нижестоящая задача (задача развертывания), в которой используется правило триггеров по умолчанию, `all_success`, указывающее на то, что задача должна выполняться только в том случае, если все ее ее вышестоящие задачи были выполнены успешно. В данном случае это неверно, поскольку родительская задача (задача условия) имеет пропущенное состояние, она не является успешной, и поэтому развертывание пропускается [[Список литературы#^ae6dac]]<c. 129>.

Перечень правил триггеров, поддерживаемых Airflow, можно найти в [[Список литературы#^ae6dac]]<c. 133>.

Airflow также позволяет обмениваться небольшими фрагментами данных между задачами с помощью механизма XCom (сокращение от cross-communication). _Идея XCom_ заключается в том, что они, по сути, _позволяют обмениваться сообщениями между задачами_, обеспечивая некоторый уровень общего состояния.

Например, при обучении модель регистрируется в реестре моделей с использованием случайно сгенерированного идентификатора. Чтобы развернуть обученную модель, нам нужно каким-то образом передать этот идентификатор следующей задаче, чтобы она знала какую версию модели следует развернуть.
 
Можно явно публиковать значения XCom явно в своей задаче с помощью метода `xcom_push()`, который доступен в экзмепляре задачи в контексте Airflow.
```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    context["task_instance"].xcom_push(key="model_id", value=model_id)  # NB!

train_model = PythonOperator(
	task_id="train_model",
	python_callable=_train_modal,
)
```

Этот вызов `xcom_push` по сути сообщает Airflow о необходимости регистрации значения `model_id` в качестве значения XCom для соответствующей задачи и соответствующего ОАГ и даты выполнения.

Получить значение XCom в других задачах можно с помощью метода `xcom_pull`, который является обратной версией метода `xcom_push`.
```python
def _deploy_model(**context):
    model_id = context["task_instance"].xcom_pull(
        task_ids="train_model",
        key="model_id",
    )
    print(f"Deploying model {model_id}")

deploy_model = PythonOperator(
	task_id="deploy_model",
	python_callable=_deploy_model,
)
```

Этот код сообщает Airflow, что нужно извлечь значение XCom с ключом `model_id` из задачи `train_model`, соответствующий `model_id`, который мы ранее поместили в задачу `train_model`. Обратите внимание, что метод `xcom_pull` также позволяет определять `dag_id` и дату выполнения при извлечении значений XCom. 

По умолчанию для этих параметров задан текущий ОАГ и дата выполнения, чтобы `xcom_pull` извлекал только значения, опубликованные текущим запуском ОАГ.

NB! Теоретически можно указать иные значения для получения значений из других ОАГ или иные даты выполнения, ==но настоятельно рекомендуется не делать этого без веских причин==.

 Также можно ссылаться на переменные XCom в шаблонах
```python
def _deploy_model(templates_dict, **context):
    model_id = templates_dict["model_id"]
    print(f"Deeploying model {model_id}")

deploy_model = PythonOperator(
	task_id="deploy_model",
	python_callable=_deploy_model,
	templates_dict={
        "model_id": "{{task_instance.xcom_pull(task_ids='train_model', key='model_id')}}"
	},
)
```

Некоторые операторы также поддерживают автоматическую передачу значений XCom. Например, у оператора `BashOperator` есть параметр `xcom_push`,  который, если для него задано значение true, сообщает оператору поместить последнюю строку, записанную в stdout командой bash в качестве значения XCom. Точно так же оператор `PythonOperator` опубликует любое значение, возвращаемое из вызываемого объекта Python как значение XCom. Это означает, что наш пример также можно записать так
```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    return model_id
```

Мы регистрируем XCom с ключом по умолчанию `return_value`, что можно увидеть, заглянув в раздел администратора.

NB! Хотя данный механизм может показаться довольно полезным для разделения состояния между задачами, его использование также  имеет некоторые недостатки. Например, один из них заключается в том, что ==он добавляет скрытую зависимость между задачами, поскольку задача по извлечению (pulling task) неявно зависит от задачи, размещающей требуемое значение==. В отличие от явных зависимостей задач, ==эту зависимость не видно в ОАГ, и она не будет учитываться при планировании задач==. Таким образом, вы несете ответственность за то, чтобы задачи с зависимостями XCom выполнялись в правильном порядке. Airflow не сделает это за вас. ==Такие скрытые зависимости становятся еще более сложными при обмене значениями XCom между разными ОАГ или датами выполнения, что также не является рекомендуемой практикой==

NB! Механизм XCom еще может быть своего рода антипаттерном, когда он нарушает атомарность оператора [[Список литературы#^ae6dac]]<c. 137>.

Наконец, техническое ограничение XCom состоит в том, что ==любое значение, хранимое XCom, должно поддерживать сериализацию==. Кроме того, размер занчения XCom может быть ограничен бэкендом, используемым для их хранения. По умолчанию они хранятся в базе метаданных Airflow и имеют следующее ограничение по размеру:
- SQLite -- хранятся как тип BLOB, ограничение 2 Гб,
- PostgreSQL -- хранятся как тип BYTEA, ограничение 1 Гб,
- MySQL -- хранятся как тип BLOB, ограничение 64 Кб.

При этом XCom может быть мощным инструментом при правильном использовании. Просто убедитесь, что вы тщательно продумали его использование и четко задокументировали зависимости, которые он вводит между задачами, чтобы избежать сюрпризов в будущем.

Ограничение при использовании базы метаданных Airflow для хранения значений XCom заключается в том, что в целом оно плохо масштабируется, когда речь идет о больших данных. Это означает, что обычно нужно использовать _XCom для хранения отдельных значений или небольших результатов_, ==но не больших наборов данных==.

В Airflow 2 появилась возможность указать настраиваемый XCom-бэкенд для развертывания в Airflow. По сути, эта опция позволяет определить собственный класс, который Airflow будет использовать для хранения и получения XCom'ов. Единственное требование -- этот класс наследует от базового класса `BaseXCom` и реализует два статических метода для сериализации и десериализации значений соответственно.
```python
from typing import Any
from airflow.models.xcom import BaseXCom

class CustomXComBackend(BaseXCom):
    @staticmethod
    def serialize_value(value: Any):
        ...
    
    @staticmethod
    def deserialize_value(result) -> Any:
        ...
```

В этом классе метод сериализации вызывается всякий раз, когда значение XCom помещается внутри оператора, тогда как метод десиреализации вызывается, когда значения XCom извлекаются из бэкенда. Чтобы использовать этот класс, нужно воспользоваться параметром `xcom_backend` в конфигурации Airflow.

Настраиваемые XCom бэкенды значительно расширяют возможности для хранения значений XCom. Например, если вы хотите хранить более крупные значения в относительно дешевом и масштабируемом облачном хранилище, то можно реализовать настраиваемый бэкенд для облачных сервисов, таких как хранилище Azure BLOB, Amazon S3 или Google GCS.  
### Связывание задач Python с помощью Taskflow API

Хотя XCom можно использовать для обмена данными между задачами Python, API может быть громоздким в использовании, особенно если вы объединяете в цепочку большое количество задач. Чтобы решить эту проблему, Airflow 2 добавил новый API на основе декоратора для определения задач Python и их зависимостей -- Taskflow API. 

Taskflow API может значительно упростить код, если вы в основном используете оператор `PythonOperator` и передаете данные между ними в виде XCom'ов.
#### Упрощение задач Python с помощью Taskflow API

Пример определения задачи обучения и развертывания с использованием обычного API
```python
def _train_model(**context):
    model_id = str(uuid.uuid4())
    context["task_instance"].xcom_push(key="model_id", value=model_id)

def _deploy_model(**context):
    model_id = context["task_instance"].xcom_pull(
        task_ids="train_model",
        key="model_id",
    )
    print(f"Deploying model {model_id}")

with DAG(...) as dag:
    ...
    train_model = PythonOperator(
        task_id="train_model",
        python_callable=_train_model, 
    )

    deploy_model = PythonOperator(
        task_id="deploy_model",
        python_callable=_deploy_model,
    )

    ...
    join_datasets >> train_model >> deploy_model
```

Недостаток этого подхода состоит в том, что сначала он требует от нас определения функции (например, `_train_model` и `_deploy_model`), которые затем нужно обернуть в `PythonOperator` для создания задачи Airflow. Более того, чтобы передавать идентификатор модели между двумя задачами, нужно явно использовать методы `xcom_push` и `xcom_pull` в функциях для отправки или получения  значения идентификатора модели. 

Taskflow API призван упростить определение этого типа (на основе `PythonOperator`) задачи, облегчив преобразование функций Python в задачи и сделав обмен переменными через механизм XCom между этими задачами более явным в определении ОАГ. 
```python
...
from airflow.decorators import task
...

with DAG(...) as dag:
    ...
    @task
    def train_model():  # -> model_id
        model_id = str(uuid.uuid4())
        return model_id  # XCom-значение, передаваемое следующей задаче
```

Обратите внимание, что мы больше не отправляем явно идентификатор модели в виде XCom, а _просто возвращаем его из функции, чтобы Taskflow API позаботился о передаче его следующей задаче_.
```python
@task
def deploy_model(model_id):  # model_id <- train_model
    print(f"Deployinng model {model_id}")
```
Здесь идентификатор модели также больше не извлекается с помощью метода `xcom_pull`, а просто передается функции Python в качестве аргумента. Теперь осталось только соединить две задачи
```python
model_id = train_model()
deploy_model(model_id)
```

По сути, когда мы вызываем декорированную функцию `train_model`, она создает новый экземпляр оператора для задачи `train_model`. По оператору `return` из функции `train_model` Airflow распознает, что мы возвращаем значение, которые автоматически будет зарегистрировано в качестве XCom, возвращаемого из задачи. Для задачи `deploy_model` мы также вызываем декорированную функцию для создания экземпляра оператора, но теперь мы еще передаем вывод `model_id` из задачи `train_model`. При этом мы фактически сообщаем Airflow, что вывод должен быть передан в качестве аргумента декорированной функции `deploy_model`. Таким образом, Airflow поймет, что между двумя задачами существует зависимость, и позаботится о передаче занчений XCom между двумя задачами.

Однако один из недостатков Taskflow API состоит в том, что его использование в настоящее время ограничено задачами Python, которые в противном случае были бы реализованы с помощью `PythonOperator`. Таким образом, задачи с участием любых других операторов Airflow потребуют использования обычного API для определения задач и их зависимостей.
