_Сенсоры_ -- представляют собой особый тип (подкласс) операторов. Сенсоры непрерывано опрашивают определенные условия, чтобы определить их истинность, и если условие истинно, то все успешно. В противном случае сенсор будет ждать и повторять попытку до тех пор, пока условие не будет исинно, или время ожидания не истечет.
```python
from airflow.sensors.filesystem import FileSensor

wait_for_supermarket_1 = FileSensor(
	task_id="wait_for_supermarket_1",
    filepath="/data/supermarket1/data.csv",
)
```

Здесь `FileSensor` выполняет проверку на предмет наличия файла `/data/supermarket1/data.csv` и возвращает true, если файл уже существует. В противном случае возвращается false, и сенсор будет ждать в течение заданного периода (по умолачанию 60 секунд) и повторит попытку. У операторов (сенсоры -- это тоже операторы) и ОАГ есть настраиваемые тайм-ауты, и сенсор продолжит проверку условия, пока не истечет время ожидания [[Список литературы#^ae6dac]]<c. 149>

_Покинг_ -- так в Airflow называется запуск сенсора и проверка условия.
### Опрос пользовательских условий

Некоторые наборы данных имеют большой размер и состоят из нескольких файлов (например, `data-01.csv`, `data-02.csv` etc.). `FileSensor` поддерживает подстановочные знаки для соответствия, например, `data-*.csv`, что соответствует любому файлу, совпадающему с шаблоном. Так, если, допустим, первый файл `data-01.csv` был доставлен, а другие все еще загружаются в общее хранилище супермаркетом, `FileSensor` вернет true, и рабочий процесс продолжит выполнение задачи, что нежелательно.

Поэтому мы договорились с супермаркетами записывать файл с именем `_SUCCESS` в качестве последней части загрузки, чтобы указать, что был загружен полный набор ежедневных данных. Группа, работающая с данными, решила, что им нужно проверить наличие одного или нескольких файлов с именем `data-*.csv` и одного файла с именем `_SUCCESS`. Используя шаблоны поиска можно было бы сопоставить несколько шаблонов со сложными шаблоном, однако есть более читабельный подход -- реализовать две проверки с помощью `PythonSensor`.

`PythonSensor` похож на `PythonOperator` в том смысле, что вы предоставляете вызываемый объект Python (функция, метод etc.), который нужно выполнить. Но вызываемый объект `PythonSensor` ограничен возвратом логического значения: `true`, чтобы указать, что условие выполнено успешно, и `false`, чтобы указать, что это не так.
```python
from pathlib import Path
from airflow.sensors.python import PythonSensor

def _wait_for_supermarket(supermarket_id):
    supermarket_path = Path("/data/" + supermarket_id)
    data_files = supermarket_path.glob("data-*.csv")
    success_file = supermarket_path / "_SUCCESS"

    return data_files and success_file.exists()

wait_for_supermaket_1 = PythonSensor(
	task_id = "wait_for_supermarket_1",
	python_callable = _wait_for_supermarket,
	op_kwargs={"supermarket_id": "supermarket1"},
	dag=dag,
)
```

Сенсоры принимают аргумент `timeout`, который содержит максимальное количество секунд, в течение которого может работать. Если в начале очередного покинга количество этих секунд превысит число, заданное для `timeout`, то это приведет к сбою сенсора.

У класса `DAG` есть аргумент `concurrency`, определяющий, сколько параллельных задач разрешено в рамках этого ОАГ.
```python
dag = DAG(
	dag_id="couponing_app",
	start_date=datetime(2019, 1, 1),
	schedule_interval="0 0 * * *",
	concurrency=50,
)
```

==Как только будет достигнут глобальный предел максимального количества задач, вся ваша система встанет, что явно нежелательно==. 

Решить эту проблему можно разными способами. Класс сенсора принимает аргумент `mode`, для которого можно задать значение `"poke"` или `"reschedule"`.

По умолчанию задано значение `"poke"`, что приводит к блокировке. Это означает, что ==задача сенсора занимает слот, пока выполняется==. Время от времени она выполняет покинг, осуществляя проверку условия, а затем ==ничего не делает, но по-прежнему занимает слот==.

Режим сенсора `"rescheulde"` _освобождает слот после завершения покинга_, поэтому слот занят, _только_ пока выполняется работа.
```python
wait_for_supermarket1 = PythonSensor(
	task_id="wait_for_supermarket_1",
	python_callable=_wait_for_supermarket,
	op_kwargs={"supermarket_id": "supermarket1"},
	mode="reschedule",
	dag=dag,
)
```

NB! Сенсоры с режимом `mode = "reschedule"` освобождают свой слот после покинга, позволяя запускать другие задачи.
#### Запуск других ОАГ

Один из способов обойти повторяющиеся задачи с (почти) равной функциональностью -- разделить ОАГ на несколько более мелких ОАГ, каждый из которых берет на себя часть общего рабочего процесса. Одно из преимуществ такого способа заключается в том, что вы можете вызывать ОАГ-2 несколько раз из ОАГ-1 вместо одного ОАГ, содержащего несколько (дублированных) задач из ОАГ-2. Возможно ли это или желательно, зависит от многих факторов, таких как сложность рабочего процесса.

Данный сценарий можно реализовать с помощью оператора `TriggerDagRunOperator` [[Список литературы]]<c. 157>. Он позволяет запускать другие ОАГ, которые можно применять для разделения частей рабочего процесса.
```python
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operator.trigger_dagrun import TriggerDagRunOperator

dag1 = DAG(
	dag_id="ingest_supermarket_data",
	start_date=airflow.utils.dates_days_ago(3),
	schedule_interval="0 16 * * *",
)

for supermarket_id in range(1, 5):
    # ...
    trigger_create_metrics_dag = TriggerDagRunOperator(
        task_id=f"trigger_create_metrics_dag_supermarket_{supermarket_id}",
        trigger_dag_id="create_metrics",  # ДОЛЖЕН совпадать с `dag_id`!!!
        dag=dag1,
    )

dag2 = DAG(
	dag_id="create_metrics",  # ДОЛЖЕН совпадать с `trigger_dag_id`!!!
	start_date=airflow.utils.dates.days_ago(3),
	schedule_interval=None,
)
# ...
```

Строка, предоставленная аргументу `trigger_dag_id` оператора `TriggerDagRunOperator`, должна соответствовать `dag_id` ОАГ, который должен быть запущен. В конченом результате теперь у нас есть два ОАГ: один для приема данных из супермаркетов, второй для вычисления показателей по данным.

Airflow управляет зависимостями между задачами в рамках одного ОАГ; ==однако он не предоставляет механизма для зависимостей между ОАГ== [[Список литературы#^ae6dac]]<c. 160>.

Сенсоры -- тоже операторы [[Список литературы#^ae6dac]]<c. 149>.

Вместо того чтобы размещать выполнение с помощью `TriggerDagRunOperator`, в некоторых ситуациях, таких как обеспечение завершенного состояния для ОАГ-1, ОАГ-2 и ОАГ-3, нужно извлечь выполнение к ОАГ-4 с помощью `ExternalTaskSensor`.
```python
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task import ExternalTaskSensor

dag1 = DAG(
	dag_id="ingest_supermarket_data",
	schedule_interval="0 16 * * *",
	...
)

dag2 = DAG(
	schedule_interval="0 16 * * *",
	...
)

DummyOperator(task_id="copy_to_raw", dag=dag1) >> DummyOperator(task_id="process_supermarket", dag=dag1)

wait = ExternalTaskSensor(
	task_id="wait_for_process_supermarket",
	external_dag_id="ingest_supermarket_data",
	external_task_id="process_market", # ДОЛЖЕН совпадать с `task_id`
    dag=dag2,
)

report = DummyOperator(task_id="report", dag=dag2)
wait >> report
```

==В мире Airflow одни ОАГ ни имеют понятия о других ОАГ== [[Список литературы#^ae6dac]]<c. 161>. В случае использования `ExternalTaskSensor` требуется совпадение между ОАГ. По умолачанию `ExternalTaskSensor` просто проверяет наличие успешного состояния задачи с точно такой же датой выполнения. Теперь предположим, что у обоих ОАГ разный интервал; тогда ни о каком совпадении не может быть и речи, и, следовательно, `ExternalTaskSensor` так и не найдет соответствующую задачу.

В случае если интервалы не совпадают, можно прибегнуть к смещению, по которому `ExternalTaskSensor` должен искать задачу в другом ОАГ. Это смещение контролируется аргументом `execution_delta` в `ExternalTaskSensor`. Он ожидает объект `timedelta`, и важно знать, что он работает вразрез с вашими ожиданиями. `timedelta` вычитается из `execution_date`, а это означает, что положительная `timedelta` фактически оглядывается назад в прошлое.

Обратите внимание, что проверка задачи с помощью `ExternalTaskSensor`, где у другого ОАГ иной период интервала, например ОАГ-1 запускается раз в день, а ОАГ-2 запускается каждые 5 часов, усложняет задание подходящего значения для `execution_delta`. Для этого варианта можно предоставить функцию, возвращающую список объектов `timedelta` через аргумент `execution_date_fn` [[Список литературы#^ae6dac]]<c. 162>.
#### Запуск рабочих процессов с помощью REST API и интерфейса командной строки

Помимо запуска ОАГ из других ОАГ, их также можно запускать через REST API и интерфейс командной строки. Это может быть полезно, если требуется запускать рабочие процессы за пределами Airflow. Или данные, поступающие в случайное время в бакет S3, можно обрабатывать, задав лямбда-функцию для вызова REST API, запуская ОАГ, вместо чтобы постоянно запускать опрос сенсорами.
```bash
$ airflow dags trigger dag1
```
Или так
```bash
$ airflow dags trigger --conf '{"supermarket_id": 1}' dag1
```

Данная часть конфигурации теперь доступна во всех задачах инициированного ОАГ, запущенного через переменные контекста задачи.
```python
import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG(
	dag_id="print_dag_run_conf",
	start_date=airflow.utils.dates.days_ago(3),
	schedule_interval=None,
)

def print_conf(**context):
    print(context["dag_run"].conf)

process = PythonOperator(
	task_id="process",
	python_callable=print_conf,
	dag=dag,
)
```

Для обеспечения аналогичного результата можно использовать REST API
```bash
$ curl \
  -u admin:admin \
  -X POST \
  "http://localhost:8080/api/v1/dags/print_dag_run_conf/dagRuns" \
  -H "Content-Type: application/json" \
  -d '{"conf": {}}'

$ curl \
  -u admin:admin \
  -X POST \
  "http://localhost:8080/api/v1/dags/print_dag_run_conf/dagRuns" \
  -H "Content-Type: application/json" \
  -d '{"conf": {"supermarket": 1}}'
```

Это может быть удобно при запуске ОАГ за пределами Airflow, например из системы непрерывной интеграции и доставки.

