Пример начальной версии ОАГ события без планирования
```python
import datetime as dt
from pathlib import Path
import pandas as pd
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
	dag_id="01_unscheduled",
	start_date=dt.datetime(2019, 1, 1),  # дата запуска ОАГ
    schedule_interval=None,  # версия ОАГ без планирования
)

fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "https://localhost:5000/events"  # извлекаем и сохраняемы события из API
	),
	dag=dag,
)

def _calculate_stats(input_path, output_path):
    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()
    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)

calculate_stats = PythonOperator(
	task_id="calculate_stats",
	python_callable=_calculate_stats,
	op_kwargs={
        "input_path": "/data/events.json",
        "output_path": "/data/stats.csv",
	},
	dag=dag,
)

fetch_events >> calculate_stats  # задаем порядок выполнения
```

ОАГ Airflow можно запускать через равные промежутки времени, задав для него запланированный интервал с помощью аргумента `schedule_interval` при инициализации ОАГ. По умолчанию значение этого аргумента равно `None`. Это означает, что _ОАГ не будет запланирован и будет запускаться только при запуске вручную_ из пользовательского интерфейса или API.
### Опеределение интервалов

Для ежедневного запуска ОАГ в полночь Airflow предлагает удобный макрос `@daily`
```python
dag = DAG(
	dag_id="02_daily_schedule",
	schedule_interval="@daily", # запуск ОАГ каждый день в полночь
	start_date=dt.datetime(2019, 1, 1), # дата и время начала планирования ОАГ
)
```

Airflow также нужно знать, когда мы хотим начать выполнение ОАГ, указав дату запуска. Исходя из этой даты, Airflow запланирует первое выполнение нашего ОАГ, чтобы запустить его в первом интервале _после_  даты начала (начало + интервал). Последующие запуски будут выполняться с заданными интервалами после первого интервала.

В сочетании с ежедневным интервалом это приведет к тому, что Airflow будет запускать наш ОАГ в полночь каждого дня после первого января. Обратите внимание, что первое выполнение происходит второго января (первый интервал после даты начала), а не первого.

Если мы знаем, что проект имеет фиксированную продолжительность, то можно сообщить Airflow, чтобы он прекратил запуск ОАГ после определенной даты, используя параметр `end_date`.
```python
dag = DAG(
	dag_id="03_with_end_date",
	schedule_interval="@daily",
	start_date=dt.datetime(year=2019, month=1, day=1),
	end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Для поддержки более сложных вариантов Airflow позволяет определять интервалы, используя тот же синтаксис, что и у cron, планировщика заданий на основе времени, используемого Unix-подобными компьютерными операционными системами, таким как macOS и Linux.

Этот синтаксис состоит из 5 компонент и определяется следующим образом:
```bash
* # минута (0-59)
  * # час (0-23)
    * # день месяца (1-31)
	  * # месяц (1-12)
	    * # день недели (0-6) (с воскресенья по субботу; в некоторых системах 7 - это тоже воскресенье)
```

Символ звездочки (`*`) можно использовать вместо чисел для определения неограниченных полей. Это означает, что нас не волнует значение этого поля. Примеры
```bash
0 * * * * # ежечасно (запуск по часам)
0 0 * * * # ежедневно (запуск в полночь)
0 0 * * 0 # еженедельно (запуск в полночь в воскресенье)
```

Помимо этого, также можно определять более сложные выражения, например
```bash
0 0 1 * *  # полночь первого числа каждого месяца
45 23 * * SAT # 23:45 каждую субботу
```

Еще выражения cron позволяют определять коллекции значений с помощью запятой (`,`) для определения списка значений или тире (`-`) для определения диапазона значений. Используя этот синтаксис, мы можем создавать выражения, позволяющие запускать задания для нескольких рабочих дней или наборов часов в течение дня. Например
```bash
# означает запуск каждый понедельник, среду, пятницу в полночь
0 0 * * MON, WED, FRI
# запускать каждый будний день в полночь, а
0 0 * * MON-FRI
# означает запуск каждый день в 00:00 и 12:00.
0 0, 12 * * *
```

Есть ресурсы (например, https://crontab.guru), которые помогают в написании cron-выражений.

Макросы Airflow для часто используемых интервалов планирования:
- `@once`: один и только один раз,
- `@hourly`: запуск один раз в час в начале часа,
- `@daily`: запуск один раз в день в полночь,
- `@weekly`: запуск один раз в неделю в полночь в воскресенье утром,
- `@monthly`: запуск один раз в месяц в полночь первого числа месяца,
- `@yearly`: запуск один раз в год в полночь 1 января.

Для поддержки частотного способа расписания (например, хотим запускать ОАГ каждые 3 дня) Airflow предлагает относительные интервалы
```python
dag = DAG(
	dag_id="04_time_delta",
    # задаем расписание на базе частоты
	schedule_interval=dt.timedelta(days=3),
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Это приведет к тому, что ОАГ будет запускаться каждые три дня после даты начала. Конечно, можно также использовать данный подход для запуска ОАГ каждые 10 минут, используя `timedelta(minutes=10))` или каждые два часа (`timedelta(hours=2)`).
###  Инкрементальная обработка данных

Инкрементальная загрузка данных -- это когда мы загружаем только события соответствующего дня в каждом интервале расписания и _рассчитываем статистику лишь для новых событий_. Такой подход намного более эффективен, нежели извлечение и обработка всего набора данных, поскольку он значительно сокращает объем данных, которые необходимо обрабатывать в каждом интервале.

Чтобы внедрить инкрементальную обработку в наш рабочий процесс, нужно изменить ОАГ, чтобы скачивать данные за определенный день. Можно настроить вызов API для извлечения событий на текущую дату, включив параметры начальной и конечной дат
```bash
$ curl -O http://localhost:5000/events?start_date=2019-01-01&end_date=2019-01-02
```
Здесь параметр `start_date` является инклюзивным параметром, а `end_date` -- эксклюзивным. Это означает, что, по сути, мы получаем события, которые происходят в период между 2019-01-01 00:00:00 и 2019-01-01 23:59:59.
Мы можем реализовать это инкрементное получение данных в ОАГ, изменив bash-команду, чтобы включить в нее две даты
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "	
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
        "start_date=2019-01-01&"
        "end_date=2019-01-02"
	),
	dag=dag,
)
```
### Динамическая привязка ко времени с использованием дат выполнения

Часто важно знать в течение какого временного интервала выполняется данная задача. По этой причине Airflow предоставляет задачам дополнительные параметры, которые можно использовать, чтобы определить, в каком интервале выполняется задача.

Самый важный из этих параметров -- `execution_date`, который обозначает дату и время, в течение которых выполняется ОАГ. `execution_date` -- это не дата, а временная метка, отражающая время начала интервала, для которого выполняется ОАГ. Время окончания интервала указывается другим параметром, `next_execution_date`. Вместе эти даты определяют всю продолжительность интервала задачи.

Airflow также предоставляет параметр `previous_execution_date`, который описывает начало предыдущего интервала. 

В Airflow эти даты выполнения можно использовать, ссылаясь на них в операторах. Например, в `BashOperator` можно использовать функцию создания шаблонов, чтобы динамически включать даты выполнения в Bash-команду.
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
        "start_date={{execution_date.strftime('%Y-%m-%d')}}"
        "&end_date={{next_execution_date.strftime('%Y-%m-%d')}}"
	)
)
```

Здесь синтаксис `{{variable_name}}` -- это синтаксис шаблонов Jinja для ссылки на один из специфических параметров Airflow. Airflow также предоставляет несколько сокращенных параметров для распространенных форматов дат. Например, параметры `ds` и `ds_nodash` представляют собой разные обозначения `execution_date` в формате YYYY-MM-DD и YYYYMMDD соответственно. Аналогично `next_ds`, `next_ds_nodash`, `prev_ds` и `prev_ds_nodash` предоставляют сокращенные обозначения для следующей и предыдущей дат выполнения соответственно.
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
	)
)
```