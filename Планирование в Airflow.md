Пример начальной версии ОАГ события без планирования
```python
import datetime as dt
from pathlib import Path
import pandas as pd
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator

dag = DAG(
	dag_id="01_unscheduled",
	start_date=dt.datetime(2019, 1, 1),  # дата запуска ОАГ
    schedule_interval=None,  # версия ОАГ без планирования
)

fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "https://localhost:5000/events"  # извлекаем и сохраняемы события из API
	),
	dag=dag,
)

def _calculate_stats(input_path, output_path):
    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()
    Path(output_path).parent.mkdir(exist_ok=True)
    stats.to_csv(output_path, index=False)

calculate_stats = PythonOperator(
	task_id="calculate_stats",
	python_callable=_calculate_stats,
	op_kwargs={
        "input_path": "/data/events.json",
        "output_path": "/data/stats.csv",
	},
	dag=dag,
)

fetch_events >> calculate_stats  # задаем порядок выполнения
```

ОАГ Airflow можно запускать через равные промежутки времени, задав для него запланированный интервал с помощью аргумента `schedule_interval` при инициализации ОАГ. По умолчанию значение этого аргумента равно `None`. Это означает, что _ОАГ не будет запланирован и будет запускаться только при запуске вручную_ из пользовательского интерфейса или API.
### Опеределение интервалов

Для ежедневного запуска ОАГ в полночь Airflow предлагает удобный макрос `@daily`
```python
dag = DAG(
	dag_id="02_daily_schedule",
	schedule_interval="@daily", # запуск ОАГ каждый день в полночь
	start_date=dt.datetime(2019, 1, 1), # дата и время начала планирования ОАГ
)
```

Airflow также нужно знать, когда мы хотим начать выполнение ОАГ, указав дату запуска. Исходя из этой даты, Airflow запланирует первое выполнение нашего ОАГ, чтобы запустить его в первом интервале _после_  даты начала (начало + интервал). Последующие запуски будут выполняться с заданными интервалами после первого интервала.

В сочетании с ежедневным интервалом это приведет к тому, что Airflow будет запускать наш ОАГ в полночь каждого дня после первого января. Обратите внимание, что первое выполнение происходит второго января (первый интервал после даты начала), а не первого.

Если мы знаем, что проект имеет фиксированную продолжительность, то можно сообщить Airflow, чтобы он прекратил запуск ОАГ после определенной даты, используя параметр `end_date`.
```python
dag = DAG(
	dag_id="03_with_end_date",
	schedule_interval="@daily",
	start_date=dt.datetime(year=2019, month=1, day=1),
	end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Для поддержки более сложных вариантов Airflow позволяет определять интервалы, используя тот же синтаксис, что и у cron, планировщика заданий на основе времени, используемого Unix-подобными компьютерными операционными системами, таким как macOS и Linux.

Этот синтаксис состоит из 5 компонент и определяется следующим образом:
```bash
* # минута (0-59)
  * # час (0-23)
    * # день месяца (1-31)
	  * # месяц (1-12)
	    * # день недели (0-6) (с воскресенья по субботу; в некоторых системах 7 - это тоже воскресенье)
```

Символ звездочки (`*`) можно использовать вместо чисел для определения неограниченных полей. Это означает, что нас не волнует значение этого поля. Примеры
```bash
0 * * * * # ежечасно (запуск по часам)
0 0 * * * # ежедневно (запуск в полночь)
0 0 * * 0 # еженедельно (запуск в полночь в воскресенье)
```

Помимо этого, также можно определять более сложные выражения, например
```bash
0 0 1 * *  # полночь первого числа каждого месяца
45 23 * * SAT # 23:45 каждую субботу
```

Еще выражения cron позволяют определять коллекции значений с помощью запятой (`,`) для определения списка значений или тире (`-`) для определения диапазона значений. Используя этот синтаксис, мы можем создавать выражения, позволяющие запускать задания для нескольких рабочих дней или наборов часов в течение дня. Например
```bash
# означает запуск каждый понедельник, среду, пятницу в полночь
0 0 * * MON, WED, FRI
# запускать каждый будний день в полночь, а
0 0 * * MON-FRI
# означает запуск каждый день в 00:00 и 12:00.
0 0, 12 * * *
```

Есть ресурсы (например, https://crontab.guru), которые помогают в написании cron-выражений.

Макросы Airflow для часто используемых интервалов планирования:
- `@once`: один и только один раз,
- `@hourly`: запуск один раз в час в начале часа,
- `@daily`: запуск один раз в день в полночь,
- `@weekly`: запуск один раз в неделю в полночь в воскресенье утром,
- `@monthly`: запуск один раз в месяц в полночь первого числа месяца,
- `@yearly`: запуск один раз в год в полночь 1 января.

Для поддержки частотного способа расписания (например, хотим запускать ОАГ каждые 3 дня) Airflow предлагает относительные интервалы
```python
dag = DAG(
	dag_id="04_time_delta",
    # задаем расписание на базе частоты
	schedule_interval=dt.timedelta(days=3),
    start_date=dt.datetime(year=2019, month=1, day=1),
    end_date=dt.datetime(year=2019, month=1, day=5),
)
```

Это приведет к тому, что ОАГ будет запускаться каждые три дня после даты начала. Конечно, можно также использовать данный подход для запуска ОАГ каждые 10 минут, используя `timedelta(minutes=10))` или каждые два часа (`timedelta(hours=2)`).
###  Инкрементальная обработка данных

Инкрементальная загрузка данных -- это когда мы загружаем только события соответствующего дня в каждом интервале расписания и _рассчитываем статистику лишь для новых событий_. Такой подход намного более эффективен, нежели извлечение и обработка всего набора данных, поскольку он значительно сокращает объем данных, которые необходимо обрабатывать в каждом интервале.

Чтобы внедрить инкрементальную обработку в наш рабочий процесс, нужно изменить ОАГ, чтобы скачивать данные за определенный день. Можно настроить вызов API для извлечения событий на текущую дату, включив параметры начальной и конечной дат
```bash
$ curl -O http://localhost:5000/events?start_date=2019-01-01&end_date=2019-01-02
```
Здесь параметр `start_date` является инклюзивным параметром, а `end_date` -- эксклюзивным. Это означает, что, по сути, мы получаем события, которые происходят в период между 2019-01-01 00:00:00 и 2019-01-01 23:59:59.
Мы можем реализовать это инкрементное получение данных в ОАГ, изменив bash-команду, чтобы включить в нее две даты
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "	
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
        "start_date=2019-01-01&"
        "end_date=2019-01-02"
	),
	dag=dag,
)
```
### Динамическая привязка ко времени с использованием дат выполнения

Часто важно знать в течение какого временного интервала выполняется данная задача. По этой причине Airflow предоставляет задачам дополнительные параметры, которые можно использовать, чтобы определить, в каком интервале выполняется задача.

Самый важный из этих параметров -- `execution_date`, который обозначает дату и время, в течение которых выполняется ОАГ. `execution_date` -- это не дата, а временная метка, отражающая время начала интервала, для которого выполняется ОАГ. Время окончания интервала указывается другим параметром, `next_execution_date`. Вместе эти даты определяют всю продолжительность интервала задачи.

NB! Параметры `execution_date` (следует использовать `logical_date`) и `next_execution_date` (следует использовать `data_interval_end`) считаются устаревшими (deprecated). См. https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html 

Airflow также предоставляет параметр `previous_execution_date`, который описывает начало предыдущего интервала. 

В Airflow эти даты выполнения можно использовать, ссылаясь на них в операторах. Например, в `BashOperator` можно использовать функцию создания шаблонов, чтобы динамически включать даты выполнения в Bash-команду.
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
        "start_date={{execution_date.strftime('%Y-%m-%d')}}"
        "&end_date={{next_execution_date.strftime('%Y-%m-%d')}}"
	)
)
```

Здесь синтаксис `{{variable_name}}` -- это синтаксис шаблонов Jinja для ссылки на один из специфических параметров Airflow. Airflow также предоставляет несколько сокращенных параметров для распространенных форматов дат. Например, параметры `ds` и `ds_nodash` представляют собой разные обозначения `execution_date` в формате YYYY-MM-DD и YYYYMMDD соответственно. Аналогично `next_ds`, `next_ds_nodash`, `prev_ds` и `prev_ds_nodash` предоставляют сокращенные обозначения для следующей и предыдущей дат выполнения соответственно.
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://localhost:5000/events?"
	)
)
```

Эту более короткую версию легче читать. Однако для более сложных форматов даты (или времени и даты) вам, вероятно, все равно придется использовать более гибкий метод `.strftime`.
### Разделение данных

Команду инкрементального извлечения данных можно записать так
```python
# ПЛОХОЙ пример, так как каждая новая задача будет перезаписывать events.json
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data && "
        "curl -o /data/events.json "
        "http://locallhost:5000/evetns?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
	),
	dag=dag,
)
```

Здесь каждая новая задача просто перезаписывает результат предыдущего дня, а означает, что фактически мы не создаем никакой истории.

Есть альтернативный подход -- разделить набор данных на ежедневные пакеты, записав вывод задачи в файл с именем соответствующей даты выполнения.
```python
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data/events && "
        "curl -o /data/events/{{ds}}.json "
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
	),
	dag=dag,
)
```
Это приведет к тому, что все данные, скачиваемые с датой выполнения 01/01/2019, будут записаны в файл `/data/events/2019-01-01.json`. 

Такая практика разделения набора данных на более мелкие и управляемые части является распространенной стратегией в системе хранения и обработки данных и обычно называется _секционированием_ (партиционированием), когда более мелкие фрагменты данных устанавливают _секции_.

Однако, используя наш секционированный набор данных, мы можем более эффективно рассчитать эту статистику для каждой отдельной секции, изменив пути ввода и вывода этой задачи, чтобы указать на секционированные данные событий и секционированный выходной файл.
```python
def _calculate_stats(**context):
    """Расчет статистики."""
    # получаем шаблонные значение из объекта `templates_dict`
    input_path = context["templates_dict"]["input_path"]
    output_path = context["templates_dict"]["ouput_path"]

    Path(output_path).parent.mkdir(exist_ok=True)

    events = pd.read_json(input_path)
    stats = events.groupby(["date", "user"]).size().reset_index()
    stats.to_csv(output_path, index=False)

calculate_stats = PythonOperator(
	task_id="calculate_stats",
	python_callable=_calculate_stats,
	templates_dict={
        "input_path": "/data/events/{{ds}}.json",
        "output_path": "/data/stats/{{ds}}.csv",
	},
	dag=dag,
)
```

В `PythonOperator` нужно передать все аргументы, которые должны быть шаблонизированы с помощью  параметра оператора `templates_dict`. Затем мы можем получить шаблонные значения внутри нашей функуции из контекстного объекта, который Airflow передает функции `_calculate_stats`.

Мы можем контролировать, когда Airflow запускает ОАГ, с помощью трех параметров:
- даты начала,
- интервала,
- даты окончания (необязательно).

Преимущество использования подхода на основе интервалов состоит в том, что он идеально подходит для выполнения инкрементальной обработки данных, поскольку мы точно знаем, в течение какого интервала времени выполняется задача, -- начало и конец соответствующего интервала. 

Это резко контрастирует с такой системой планирования, как `cron`, где нам известно только текущее время, в течение которого выполняется задача. Это означает, что, например, в `cron` мы должны либо вычислить, либо угадать, где остановилось предыдущее выполнение, предполагая, что задача выполняется за предыдущий день.

Когда даты выполнения Airflow определены как начало соответствующих интервалов, их можно использовать для определения начала и конца определенного  интервала. Например, при выполнении задачи начало и конец соответствующего интервала определяются параметрами `execution_date` (начало интервала) и `next_execution_date` (начало следующего интервала).

В Airflow дата выполнения ОАГ определяется как время _начала соответствующего интервала_. Таким образом, значение `execution_date` _указывает на начало текущего интервала_, в то время как параметры `previous_execution_date` и `next_execution_date` указывают на начало предыдущего и следующего интервалов соответственно [[Список литературы#^ae6dac]]<c. 82>.
### Использование обратного заполнения

Поскольку Airflow позволяет определять интервалы с произвольной даты начала, мы также можем определить интервалы от даты начала в прошлом. Можно использовать это свойство для выполнения запусков ОАГ для загрузки или анализа наборов архивных данных -- это процесс, который обычно называют _бэкфиллингом_, или _обратным заполнением_.

По умолчанию Airflow будет планировать и запускать все прошлые интервалы, которые не были выполнены. Таким образом, указание даты начала в прошлом и активации соответствующего ОАГ даст интервалы, которые прошли до выполнения текущего времени. Такое поведение контролируется параметром ОАГ `catchup`, и его можно отключить, задав для него значение `False`.
```python
# catchup=False чтобы подавить архивное выполнение запусков
dag = DAG(
	dag_id="09_no_catchup",
	schedule_interval="@daily",
	start_date=dt.datetime(year=2019, month=1, day=1),
	end_date=dt.datetime(year=2019, month=1, day=5),
    catchup=False,  # NB!
)
```
При такой настройке ОАГ будет запускаться только с учетом последнего интервала, вместо того чтобы выполнять все открытые интервалы в прошлом. Значением `catchup` можно управлять из конфигурационного файла Airflow, задав значение для параметра конфигурации `catchup_by_default` [[Список литературы#^ae6dac]]<c. 83>.

==По умолчанию Airflow будет запускать задачи для всех прошедших интервалов до текущего времени==. Такое поведение можно отключить, задав для параметра `catchup` значение `False`, и _в этом случае Airflow начнет выполнять задачи только с текущего интервала_.

==Поскольку API обратного заполнения предоставляет только до 30 дней в прошлом, обратное заполнение нельзя использовать для загрузки данных за более ранние дни==.

Обратное заполнение также можно применять для повторной обработки данных, после того как мы внесли изменения в наш код. Например, предположим, что мы вносим изменения в функцию `calc_statistics`, чтобы добавить новую статистику. Используя обратное заполнение, можно очистить прошлые запуски нашей задачи `calc_statistics`, чтобы повторно проанализировать архивные данные с помощью нового кода. _И в этом случае мы не ограничены 30-дневным лимитом источника данных, поскольку мы уже загрузили эти более ранние секции данных в рамках прошедших запусков_.
### Лучшие практики для проектирования задач
#### Атомарность

Термин атомарность часто используется в системах баз данных, где _атомарная транзакция_ считается _неделимой_  и несводимой серией операций с базой данных: либо происходит все, либо не происходит ничего. 

Так же и в Airflow: задачи должны быть определены таким образом, чтобы они были успешными и давали надлежащий результат, либо терпели неудачу, не влияя на состояние системы.

Чтобы реализовать функциональность отправки электронной почты в случае успешного завершения задачи, можно просто выделить функцию отправки электронной почты в отдельную задачу
```python
def _send_stats(email, **context):
    stats = pd.read_csv(context["templates_dict"]["stats_path"])
    email_stats(stats, email=email)

send_stats = PythonOperator(
	task_id="send_stats",
	python_callable=_send_stats,
	op_kwargs={"email": "user@example.com"},
	templates_dict={"stats_path": "/data/stats/{{ds}}.csv"},
	dag=dag,
)

calculate_stats >> send_stats
```
Здесь используется и `op_kwargs` для передачи значения обычному параметру `email`, и `template_dict` для того чтобы передать значение параметра `stats_path` с учетом шаблонов Jinja.
#### Идемпотентность

Еще одно важное свойство, которое следует учитывать при написании задач Airflow, -- это _идемпотентность_. Задачи называются идемпотентными, если вызов одной и той же задачи несколько раз с одними и теми же входными данными не имеет дополнительного эффекта.

Это означает, что повторной запуск задачи без изменения входных данных не должен изменять общий результат.

Рассмотрим реализацию, которая извлекает результаты за один день и записывает их в секционированный набор данных
```python
# В BashOperator можно сразу использовать синтаксис шаблонов Jinja
fetch_events = BashOperator(
	task_id="fetch_events",
	bash_command=(
        "mkdir -p /data/events/ && "	
        "curl -o /data/events/{{ds}}.json "
        "http://localhost:5000/events?"
        "start_date={{ds}}&"
        "end_date={{next_ds}}"
	),
	dag=dag,
)
```

Повторный запуск этой задачи для заданной даты приведет к тому, что задача получит тот же набор событий, что и при предыдущем выполнении (при условии что дата находится в пределах 30-дневного окна) и перезаписи сущесвующего файла в формате JSON в файле `/data/events` с тем же результатом. Таким образом, данная реализация задачи `fetch_events` явно идемпотентна.

Чтобы показать пример неидемпотентной задачи, рассмотрим возможность использования файла JSON (`/data/events.json`), просто добавляя события в этот файл. В этом случае повторный запуск задачи приведет к тому, что события просто будут добавлены к существующему набору данных, дублируя события дня. Таким образом, эта реализация не является идемпотентной, поскольку дополнительное выполнение задачи меняет общий результат.
